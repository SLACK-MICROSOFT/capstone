{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 608kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.13.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449907 sha256=4145e1eaf4fdd4dbe1e2f2500451e22071ace88eed1ec0673b1070fa769f4912\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk \n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk import bigrams\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0                                            comment  \\\n",
      "0             0  Founded in 2017, GoChain started with an idea ...   \n",
      "1             1                     i would like to learn that too   \n",
      "2             2  TLDR margin means you borrow money to trade, y...   \n",
      "3             3  A nice short answer, thanks! Two follow-up que...   \n",
      "4             4  Yes 3x means  3 times of your asset value.\\nEg...   \n",
      "...         ...                                                ...   \n",
      "6997         69                  Traffic of what site?\\n\\n&#x200B;   \n",
      "6998         70  Ok yeah huge decrease in a year, but where exa...   \n",
      "6999         71                                     This subreddit   \n",
      "7000         72                                Got it thanks mate.   \n",
      "7001          \u001a                                                NaN   \n",
      "\n",
      "      Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  \n",
      "0            NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "1            NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "2            NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "3            NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "4            NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "...          ...         ...         ...         ...         ...         ...  \n",
      "6997         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "6998         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "6999         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "7000         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "7001         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "\n",
      "[7002 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#Need to gather all data into a long text string.We want to use df['comment'] for our comments. \n",
    "\n",
    "df = pd.read_csv('data/Comments/combined-comments.csv')\n",
    "\n",
    "#titles = df['title']\n",
    "#body = df['body']\n",
    "print(df)\n",
    "comments = df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltkprep_com = comments.str.cat(sep='; ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266774\n",
      "147064\n",
      "16194\n"
     ]
    }
   ],
   "source": [
    "#Right now, this divides it into sentences. We want to remove stopwords first \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#We want to remove all punctuation from each word. Or else .,? would be the most frequent occurrences\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(nltkprep_com)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['I', 'https', 'like', 'this', 'you', 'com', 'www', 'The', 'You'])\n",
    "\n",
    "\n",
    "print(len(tokenized_text))\n",
    "filtered_tokenized_text = [word for word in tokenized_text if word not in stopwords]\n",
    "print(len(filtered_tokenized_text))\n",
    "\n",
    "filtered_word_freq = nltk.FreqDist(filtered_tokenized_text)\n",
    "#filtered_word_freq = nltk.FreqDist(tokenized_text)\n",
    "filtered_word_freq\n",
    "\n",
    "words = filtered_word_freq.keys()\n",
    "\n",
    "#print(words)\n",
    "\n",
    "print(len(words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f02e7778630>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = filtered_word_freq.plot(20, title = 'Most Common Words in Subreddit Comments')\n",
    "\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16348\n",
      "57\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Caveman': 1,\n",
       " 'hitman': 4,\n",
       " 'strawman': 1,\n",
       " 'Chairman': 5,\n",
       " 'mans': 1,\n",
       " 'salesman': 2,\n",
       " 'middleman': 2,\n",
       " 'businessman': 1,\n",
       " 'woman': 6,\n",
       " 'man': 57}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#But what if we removed the stopwords to measure masculine/feminine pronouns? Let's look at the example below.\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_text = tokenizer.tokenize(nltkprep_com)\n",
    "\n",
    "#print(len(tokenized_text))\n",
    "\n",
    "filtered_tokenized_text = [word for word in tokenized_text]\n",
    "#print(filtered_tokenized_text)\n",
    "filtered_word_freq = nltk.FreqDist(filtered_tokenized_text)\n",
    "filtered_word_freq\n",
    "\n",
    "words = filtered_word_freq.keys()\n",
    "\n",
    "\n",
    "print(len(words))\n",
    "\n",
    "dict = {\n",
    "    'he':filtered_word_freq['he'] / len(tokenized_text),\n",
    "    'him':filtered_word_freq['him']/ len(tokenized_text),\n",
    "    'his':filtered_word_freq['his']/ len(tokenized_text),\n",
    "    'she':filtered_word_freq['she']/ len(tokenized_text),\n",
    "    'her':filtered_word_freq['her']/ len(tokenized_text),\n",
    "    'hers':filtered_word_freq['hers']/ len(tokenized_text) \n",
    "    }\n",
    "\n",
    "all_man_words = {}\n",
    "gendered_word_list = {}\n",
    "    \n",
    "for key in filtered_word_freq.keys():\n",
    "    if key[-3:] == 'man':\n",
    "        all_man_words[key] = filtered_word_freq[key]\n",
    "    elif key[:3] == 'man':\n",
    "        all_man_words[key] = filtered_word_freq[key]\n",
    "\n",
    "Interested = ['man-made', 'manpower', 'mankind',  'Caveman', 'hitman', 'strawman', 'Chairman', 'mans', 'salesman', 'middleman', 'businessman', 'woman', 'man']\n",
    "imp_phrases = []\n",
    "\n",
    "for index in Interested: \n",
    "    if index in all_man_words:\n",
    "        gendered_word_list[index] = all_man_words[index]\n",
    "\n",
    "        \n",
    "#This is a list of all words that are gendered. \n",
    "import gendered_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suddenly realizing you haven't sent the pics yet, you fire them off and hope it's enough to keep Mango Man busy while you inform your daughter that you do, in fact, want one and ask her to buy it immediately.\n",
      "An EP is an individual who has actual or constructive possession of explosive materials during the course of his or her employment with the applicant’s business.\n",
      "She should be hanged in the public square for her draconian sentencing alone let alone everything else.\n",
      "As long as a woman’s safety is ensured during her trip, the prohibition is lifted.\n",
      "An EP is an individual who has actual or constructive possession of explosive materials during the course of his or her employment with the applicant’s business.\n",
      "She should be hanged in the public square for her draconian sentencing alone let alone everything else.\n",
      "As long as a woman’s safety is ensured during her trip, the prohibition is lifted.\n",
      "\"While Loeffler said she was \"pro-Trump,\" and in favor of his border wall, the Second Amendment and against abortion and socialism, she did not address crypto or bitcoin in her introductory remarks.\n",
      "; Imagine when you will have a president who is pro bitcoin.... day of reckoning; [Let this sink in](https://f4.bcbits.com/img/a1985991742_10.jpg); You're correct about her in relation to crypto and Wall Street.\n",
      "; > While Loeffler said she was \"pro-Trump,\" and in favor of his border wall, the Second Amendment and against abortion and socialism, she did not address crypto or bitcoin in her introductory remarks.\n",
      "Furthermore, I don't see anything in her past that qualifies her to be a U.S.\n",
      "; Yea, uh, pretty obvious that she does, considering her background.\n",
      "; Now she's a billionaire (with her husband).\n",
      "I'm not saying she has all the facts or is fully aware of what her son was doing during that period, but I'd recommend hearing her out.\n",
      "Loeffler is the CEO of Bakkt and her being a US Senator should mean good things for crypto in the US.\n",
      "; Ya, she literally said she is 'pro-trump' in her public remarks.\n",
      "; Just had my girl check on her iPhone and it's working for her.\n",
      "Just go on twitter and say anything even slightly negative about it, or disagree with Tiffany Hayden, and her white knight brigade shows up.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import *\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#analyze bigrams and trigrams\n",
    "sent_tokenizer = RegexpTokenizer(r'.+')\n",
    "sent_text = sent_tokenize(nltkprep_com)\n",
    "filtered_sent_text = [sentence for sentence in sent_text]\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_text = tokenizer.tokenize(nltkprep_com)\n",
    "\n",
    "\n",
    "filtered_tokenized_text = [word for word in tokenized_text]\n",
    "#print(filtered_tokenized_text)\n",
    "filtered_word_freq = nltk.FreqDist(filtered_tokenized_text)\n",
    "filtered_word_freq\n",
    "\n",
    "words = filtered_word_freq.keys()\n",
    "\n",
    "\n",
    "bi_man = list(nltk.bigrams(filtered_sent_text))\n",
    "tri_man = list(nltk.trigrams(filtered_sent_text))\n",
    "#creates the bigrams surrounding the desired words\n",
    "\n",
    "for i in filtered_sent_text:\n",
    "    if ' her ' in i :\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "\"\"\"To find all instances of gender bias, we propose the following. \n",
    "Access the frequency of gender pronounds - 'he', 'she', 'him', 'her', 'his', 'hers'. Have the percentages saved relative to the entire word count. \n",
    "Access words that either end or begin with -'man'\n",
    "Perform sentence searches for the following words: \n",
    "'dude, bro, bitch, girl, ladies and gentlemen, woman, man, boy'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 608kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.13.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449907 sha256=4145e1eaf4fdd4dbe1e2f2500451e22071ace88eed1ec0673b1070fa769f4912\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk \n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk import bigrams\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0                                            comment  \\\n",
      "0             0  Founded in 2017, GoChain started with an idea ...   \n",
      "1             1                     i would like to learn that too   \n",
      "2             2  TLDR margin means you borrow money to trade, y...   \n",
      "3             3  A nice short answer, thanks! Two follow-up que...   \n",
      "4             4  Yes 3x means  3 times of your asset value.\\nEg...   \n",
      "...         ...                                                ...   \n",
      "6997         69                  Traffic of what site?\\n\\n&#x200B;   \n",
      "6998         70  Ok yeah huge decrease in a year, but where exa...   \n",
      "6999         71                                     This subreddit   \n",
      "7000         72                                Got it thanks mate.   \n",
      "7001          \u001a                                                NaN   \n",
      "\n",
      "      Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  \n",
      "0            NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "1            NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "2            NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "3            NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "4            NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "...          ...         ...         ...         ...         ...         ...  \n",
      "6997         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "6998         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "6999         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "7000         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "7001         NaN         NaN         NaN         NaN         NaN         NaN  \n",
      "\n",
      "[7002 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#Need to gather all data into a long text string.We want to use df['comment'] for our comments. \n",
    "\n",
    "df = pd.read_csv('data/Comments/combined-comments.csv')\n",
    "\n",
    "#titles = df['title']\n",
    "#body = df['body']\n",
    "print(df)\n",
    "comments = df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltkprep_com = comments.str.cat(sep='; ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266774\n",
      "147064\n",
      "16194\n"
     ]
    }
   ],
   "source": [
    "#Right now, this divides it into sentences. We want to remove stopwords first \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#We want to remove all punctuation from each word. Or else .,? would be the most frequent occurrences\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(nltkprep_com)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['I', 'https', 'like', 'this', 'you', 'com', 'www', 'The', 'You'])\n",
    "\n",
    "\n",
    "print(len(tokenized_text))\n",
    "filtered_tokenized_text = [word for word in tokenized_text if word not in stopwords]\n",
    "print(len(filtered_tokenized_text))\n",
    "\n",
    "filtered_word_freq = nltk.FreqDist(filtered_tokenized_text)\n",
    "#filtered_word_freq = nltk.FreqDist(tokenized_text)\n",
    "filtered_word_freq\n",
    "\n",
    "words = filtered_word_freq.keys()\n",
    "\n",
    "#print(words)\n",
    "\n",
    "print(len(words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f02e7778630>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = filtered_word_freq.plot(20, title = 'Most Common Words in Subreddit Comments')\n",
    "\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16348\n",
      "57\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Caveman': 1,\n",
       " 'hitman': 4,\n",
       " 'strawman': 1,\n",
       " 'Chairman': 5,\n",
       " 'mans': 1,\n",
       " 'salesman': 2,\n",
       " 'middleman': 2,\n",
       " 'businessman': 1,\n",
       " 'woman': 6,\n",
       " 'man': 57}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#But what if we removed the stopwords to measure masculine/feminine pronouns? Let's look at the example below.\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_text = tokenizer.tokenize(nltkprep_com)\n",
    "\n",
    "#print(len(tokenized_text))\n",
    "\n",
    "filtered_tokenized_text = [word for word in tokenized_text]\n",
    "#print(filtered_tokenized_text)\n",
    "filtered_word_freq = nltk.FreqDist(filtered_tokenized_text)\n",
    "filtered_word_freq\n",
    "\n",
    "words = filtered_word_freq.keys()\n",
    "\n",
    "\n",
    "print(len(words))\n",
    "\n",
    "dict = {\n",
    "    'he':filtered_word_freq['he'] / len(tokenized_text),\n",
    "    'him':filtered_word_freq['him']/ len(tokenized_text),\n",
    "    'his':filtered_word_freq['his']/ len(tokenized_text),\n",
    "    'she':filtered_word_freq['she']/ len(tokenized_text),\n",
    "    'her':filtered_word_freq['her']/ len(tokenized_text),\n",
    "    'hers':filtered_word_freq['hers']/ len(tokenized_text) \n",
    "    }\n",
    "                             \n",
    "print(filtered_word_freq['man'])\n",
    "\n",
    "print(filtered_word_freq['actor'])\n",
    "\n",
    "all_man_words = {}\n",
    "gendered_word_list = {}\n",
    "    \n",
    "for key in filtered_word_freq.keys():\n",
    "    if key[-3:] == 'man':\n",
    "        all_man_words[key] = filtered_word_freq[key]\n",
    "    elif key[:3] == 'man':\n",
    "        all_man_words[key] = filtered_word_freq[key]\n",
    "\n",
    "Interested = ['man-made', 'manpower', 'mankind',  'Caveman', 'hitman', 'strawman', 'Chairman', 'mans', 'salesman', 'middleman', 'businessman', 'woman', 'man']\n",
    "\n",
    "for index in Interested: \n",
    "    if index in all_man_words:\n",
    "        gendered_word_list[index] = all_man_words[index]\n",
    "\n",
    "        \n",
    "#This is a list of all words that are gendered. \n",
    "gendered_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Thanks', 'my', 'man')\n",
      "('my', 'man', 'Hi')\n",
      "('man', 'Hi', 'u')\n",
      "('removed', 'thanks', 'man')\n",
      "('thanks', 'man', 'how')\n",
      "('man', 'how', 'do')\n",
      "('hacks', 'Thanks', 'man')\n",
      "('Thanks', 'man', 'Yes')\n",
      "('man', 'Yes', 'they')\n",
      "('company', 'thanks', 'man')\n",
      "('thanks', 'man', 'looked')\n",
      "('man', 'looked', 'that')\n",
      "('all', 'you', 'man')\n",
      "('you', 'man', 'What')\n",
      "('man', 'What', 'you')\n",
      "('re', 'trippin', 'man')\n",
      "('trippin', 'man', 'lol')\n",
      "('man', 'lol', 'Don')\n",
      "('effort', 'of', 'man')\n",
      "('of', 'man', 'is')\n",
      "('man', 'is', 'shielded')\n",
      "('the', 'middle', 'man')\n",
      "('middle', 'man', 'and')\n",
      "('man', 'and', 'gives')\n",
      "('chaintip', 'Fuck', 'man')\n",
      "('Fuck', 'man', 'That')\n",
      "('man', 'That', 's')\n",
      "('just', 'crazy', 'man')\n",
      "('crazy', 'man', 'Socialism')\n",
      "('man', 'Socialism', 'is')\n",
      "('the', 'support', 'man')\n",
      "('support', 'man', 'That')\n",
      "('man', 'That', 'core')\n",
      "('shitcoin', 'Ver', 'man')\n",
      "('Ver', 'man', 'bad')\n",
      "('man', 'bad', 'SHUT')\n",
      "('your', 'dreams', 'man')\n",
      "('dreams', 'man', 'Thanks')\n",
      "('man', 'Thanks', 'for')\n",
      "('dream', 'alive', 'man')\n",
      "('alive', 'man', 'Obviously')\n",
      "('man', 'Obviously', 'you')\n",
      "('process', 'Good', 'man')\n",
      "('Good', 'man', 'Rodger')\n",
      "('man', 'Rodger', 'I')\n",
      "('not', 'one', 'man')\n",
      "('one', 'man', 'in')\n",
      "('man', 'in', 'a')\n",
      "('under', 'severed', 'man')\n",
      "('severed', 'man', 'You')\n",
      "('man', 'You', 'troll')\n",
      "('cryptochecker', 'Sure', 'man')\n",
      "('Sure', 'man', 'nothing')\n",
      "('man', 'nothing', 'changed')\n",
      "('without', 'a', 'man')\n",
      "('a', 'man', 'and')\n",
      "('man', 'and', 'can')\n",
      "('the', 'middle', 'man')\n",
      "('middle', 'man', 'and')\n",
      "('man', 'and', 'gives')\n",
      "('chaintip', 'Fuck', 'man')\n",
      "('Fuck', 'man', 'That')\n",
      "('man', 'That', 's')\n",
      "('just', 'crazy', 'man')\n",
      "('crazy', 'man', 'Socialism')\n",
      "('man', 'Socialism', 'is')\n",
      "('the', 'support', 'man')\n",
      "('support', 'man', 'That')\n",
      "('man', 'That', 'core')\n",
      "('shitcoin', 'Ver', 'man')\n",
      "('Ver', 'man', 'bad')\n",
      "('man', 'bad', 'SHUT')\n",
      "('your', 'dreams', 'man')\n",
      "('dreams', 'man', 'Thanks')\n",
      "('man', 'Thanks', 'for')\n",
      "('dream', 'alive', 'man')\n",
      "('alive', 'man', 'Obviously')\n",
      "('man', 'Obviously', 'you')\n",
      "('process', 'Good', 'man')\n",
      "('Good', 'man', 'Rodger')\n",
      "('man', 'Rodger', 'I')\n",
      "('not', 'one', 'man')\n",
      "('one', 'man', 'in')\n",
      "('man', 'in', 'a')\n",
      "('under', 'severed', 'man')\n",
      "('severed', 'man', 'You')\n",
      "('man', 'You', 'troll')\n",
      "('cryptochecker', 'Sure', 'man')\n",
      "('Sure', 'man', 'nothing')\n",
      "('man', 'nothing', 'changed')\n",
      "('without', 'a', 'man')\n",
      "('a', 'man', 'and')\n",
      "('man', 'and', 'can')\n",
      "('give', 'this', 'man')\n",
      "('this', 'man', 'a')\n",
      "('man', 'a', 'silver')\n",
      "('off', 'O', 'man')\n",
      "('O', 'man', 'I')\n",
      "('man', 'I', 'did')\n",
      "('NEMesis', 'Oh', 'man')\n",
      "('Oh', 'man', 'I')\n",
      "('man', 'I', 'was')\n",
      "('ask', 'this', 'man')\n",
      "('this', 'man', 'questions')\n",
      "('man', 'questions', 'everything')\n",
      "('No', 'middle', 'man')\n",
      "('middle', 'man', 'that')\n",
      "('man', 'that', 's')\n",
      "('kind', 'of', 'man')\n",
      "('of', 'man', 'I')\n",
      "('man', 'I', 'would')\n",
      "('space', 'time', 'man')\n",
      "('time', 'man', 'power')\n",
      "('man', 'power', 'and')\n",
      "('to', 'take', 'man')\n",
      "('take', 'man', 'to')\n",
      "('man', 'to', 'proxima')\n",
      "('work', 'my', 'man')\n",
      "('my', 'man', 'Edit')\n",
      "('man', 'Edit', 'fixed')\n",
      "('trillion', 'dollar', 'man')\n",
      "('dollar', 'man', 'last')\n",
      "('man', 'last', 'year')\n",
      "('stuff', 'my', 'man')\n",
      "('my', 'man', 'Its')\n",
      "('man', 'Its', 'funny')\n",
      "('let', 'the', 'man')\n",
      "('the', 'man', 'go')\n",
      "('man', 'go', 'free')\n",
      "('noise', 'Hey', 'man')\n",
      "('Hey', 'man', 'saw')\n",
      "('man', 'saw', 'your')\n",
      "('it', 'up', 'man')\n",
      "('up', 'man', 'I')\n",
      "('man', 'I', 'll')\n",
      "('now', 'Hey', 'man')\n",
      "('Hey', 'man', 'love')\n",
      "('man', 'love', 'the')\n",
      "('No', 'problem', 'man')\n",
      "('problem', 'man', 'I')\n",
      "('man', 'I', 'm')\n",
      "('hot', 'my', 'man')\n",
      "('my', 'man', 'total')\n",
      "('man', 'total', 'after')\n",
      "('around', 'Ya', 'man')\n",
      "('Ya', 'man', 'It')\n",
      "('man', 'It', 'doesn')\n",
      "('Nice', 'rig', 'man')\n",
      "('rig', 'man', '8')\n",
      "('man', '8', 'x')\n",
      "('Hell', 'yeah', 'man')\n",
      "('yeah', 'man', 'thanks')\n",
      "('man', 'thanks', 'for')\n",
      "('nice', 'ones', 'man')\n",
      "('ones', 'man', 'Minimal')\n",
      "('man', 'Minimal', 'fans')\n",
      "('better', 'Hey', 'man')\n",
      "('Hey', 'man', 'I')\n",
      "('man', 'I', 'm')\n",
      "('sure', 'my', 'man')\n",
      "('my', 'man', 'I')\n",
      "('man', 'I', 'hope')\n",
      "('tonight', 'thanks', 'man')\n",
      "('thanks', 'man', 'Hi')\n",
      "('man', 'Hi', 'i')\n",
      "('I', 'understand', 'man')\n",
      "('understand', 'man', 'Sorry')\n",
      "('man', 'Sorry', 'your')\n",
      "('free', 'electricity', 'man')\n",
      "('electricity', 'man', 'its')\n",
      "('man', 'its', 'still')\n"
     ]
    }
   ],
   "source": [
    "#analyze bigrams and trigrams\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_text = tokenizer.tokenize(nltkprep_com)\n",
    "\n",
    "\n",
    "filtered_tokenized_text = [word for word in tokenized_text]\n",
    "#print(filtered_tokenized_text)\n",
    "filtered_word_freq = nltk.FreqDist(filtered_tokenized_text)\n",
    "filtered_word_freq\n",
    "\n",
    "words = filtered_word_freq.keys()\n",
    "\n",
    "p = bigrams(['ladies', 'and', 'gentlemen'])\n",
    "\n",
    "p = list(nltk.trigrams(filtered_tokenized_text))\n",
    "\n",
    "for i in p:\n",
    "    if 'man' in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
